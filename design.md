# FPGAOL Design Overview

在[FPGAOL](http://202.38.79.134)开发的过程中，我们不断地优化系统的性能，力图为用户提供和线下使用FPGA相同甚至更好的体验。我们进行了很多摸索，最终在有限的条件下支撑起了几十台FPGA节点的负荷。本文是对FPGAOL整体设计的总结，讨论了我们在一些重要问题上的思考过程和创新点，可以为日后的开发者或是希望深入了解FPGAOL项目的用户提供参考。

## 整体架构

FPGAOL采用了典型的Master-Worker的架构，一个**中心服务器**负责监控集群中各节点的工作状态并给用户分配FPGA资源，每个**节点**由一台树莓派和一个FPGA组成。用户通过[HTTP](https://tools.ietf.org/html/rfc2616)访问中心服务器并通过[WebSocket](https://tools.ietf.org/html/rfc6455)实时控制树莓派对FPGA进行烧写和输入输出。下图为我们的FPGA集群的一角。

<img src="img/cluster.jpeg" alt="FPGA borad" width="400" />

### Scale Up：减轻服务器的压力

从之前的描述可知，中心服务器是FPGAOL面对用户的接口，还负责控制整个集群，承担着比较大的压力。有趣的是，我们的服务器是一台十年前的奔腾双核台式机，性能并不出色。因此，从FPGAOL设计的初期，减轻服务器的压力，让我们的服务能轻松地从几台FPGA的规模扩展到几十台再到更多，一直是我们考虑的问题。

我们的解决方法是**将尽可能多的功能下放到集群中的树莓派上**。为此，我们在每台树莓派上都运行了一个Web Server，用户对FPGA的烧写和控制信号直接通过中心服务器上的**TCP转发**到达树莓派，树莓派对FPGA的采样结果也是直接发给用户。这样，中心服务器实际上充当了**网关**的角色，它不需要对用户和树莓派之间的通信进行任何处理，而只需要对网络数据包进行转发（每秒约1KB），计算开销很小。经过实验，同时工作的FPGA节点增加时，中心服务器的load average几乎不变。

在最新的FPGAOL v1.1版本中，我们还将渲染FPGA的控制页面的工作也从中心服务器下放到了树莓派上，这样，每个结点就完全实现了独立运行服务，与中心服务器无关。

### 高效利用网络带宽

FPGAOL的网络拓扑是一个树形结构，如下图所示：

<img src="img/topo.png" alt="Network Topology" width="400" />

尽管用户和树莓派之间的通信流量很小（1kb/s），但用户上传比特流的大小约为4MB，因此，一旦出现很多用户同时上传比特流的情况，网络的瓶颈处（汇聚交换机和接入交换机之间的百兆网）可能形成拥塞，**这会使得其他用户和树莓派间的实时通信出现丢包、卡顿**，严重影响服务质量，主要有以下两个解决方法：

* 按接入交换机对结点进行分组和**负载均衡**，尽量避免同一个分组中的多个结点同时上传比特流。
* 对用户上传的比特流文件进行压缩，减小带宽占用，经试验，可以将比特流压缩为几KB，效果较好。

## Master Node：中心服务器

如前一部分所述，FPGAOL的中心服务器是比较轻量的。除了为整个平台提供Web页面之外，它只负责对集群的管理和分配。

### 基于token的访问控制



### Failure (& Recovery) Detection



## Worker Nodes：树莓派-FPGA节点

FPGAOL集群中的每个节点由一个树莓派和一个FPGA组成，下图为FPGAOL v1.1中的一个节点：

<img src="img/board.jpeg" alt="FPGA board" width="500" />

为了使FPGAOL更加可扩展，我们不断地去中心化，充分发挥树莓派的计算能力。

### 基于DMA的高速采样



### 七段数码管的显示算法

